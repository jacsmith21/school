\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}

\title{Generative Adversarial Networks}
\author{Jacob Smith}
\date{December 12, 2017}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage[options ]{algorithm2e}
\usepackage{parskip}

\graphicspath{{./img/}}

\begin{document}
\maketitle

\section{Introduction}
This report details the experimental study conducted for the CS6735 programming project. Six machine learning algorithms were implemented, including both regular and ensemble learning algorithms. Each was evaluated on five datasets from the UCI machine learning data repository using 10 times 5-fold cross validation. The technical details of the implementation will be described and the results will be analyzed.

\section{Objectives}
\begin{enumerate}
  \item Implement the following machine learning algorithms:
  \begin{enumerate}
    \item ID3
    \item Adaboost on ID3
    \item Random Forest
    \item Naïve Bayes
    \item Adaboost on Naïve Bayes
    \item K-nearest neighbors
  \end{enumerate}
  \item Evaluate the algorithms on the following datasets:
  \begin{enumerate}
    \item Brest Cancer Data
    \item Car Data
    \item Ecoli Data
    \item Mushroom Data
  \end{enumerate}
  \item Create report of the experimental study including:
  \begin{enumerate}
    \item Description of the learning algorithms you implement.
    \item Description of the datasets you use (number of examples, number of attribute, number of classes, type of attributes, etc.).
    \item Technical details of your implementation: pre-processing of data sets (discretization, etc.), parameter setting, etc.
    \item Design of your programming implementation (data structures, overall program structure).
    \item Report and analysis of your experimental results using using 10 times 5-fold cross-validation (include mean, standard deviation of accuracies).
    \item Comparison and discussion of the algorithms with respect to the experimental results.
  \end{enumerate}
\end{enumerate}

\section{Algorithms}
For this study, six different algorithms; however, only the five unique algorithms are described in this section. Both the ID3 decision tree and Naïve Bayes algorithms were applied to the adaboost ensemble algorithm as weak learners.

\subsection{ID3}
The Iterative Dichotomiser 3 (ID3) algorithm is a decision tree algorithm used to efficiently develop decision trees \cite{Quinlan:1986:IDT:637962.637969}. This algorithm was developed for situations where there are large amount of attributes and training sets which contain many attributes, but there was the need to be able to efficiently develop relatively well performing decision trees.

The algorithm as described by Mitchell \cite{Mitchell:1997:ML:541177} was implmented. The operations begin at the root node with a set of catigorical, labled data $S$. The attribute $A$ which introduces the largest information gain is chosen to split on. This is an exhaustive process which checks all remaining attributes. This process imediatly stops if there are no possible subsets (identical samples have same class label) or the split is not statistically significant (positive information gain). If these stopping conditions are not met, the data $S$ is split using attribute $A$ into subsets ${S_1,...,S_n}$ where $n$ is the amount of different categories of attribute $A$. A new node is then created for each subset. If the entropy of a new node is $0$ or the sample count is $1$, then the node is turned into a leaf and the splitting operation ends. Furthermore, two additional early stopping conditions are implemented. Max tree level and minimum number of samples attributes are initially set at the start of training.

After the initial ID3 algorithm was implemented, furthure enhancements were made to allow for continuous data to be used. This enhancement was taken from the C4.5  as developed by Ross Quinlan \citep{c45algorithm}. Given a continuous attribute $A$, we temporarily sort the data $S$. After the data is sorted, we iteratively determine a threshhold to split the data as the maximize the information gain. To test potential threshholds, we find two sequential numbers which are not of the same category and split at the midway point of the two values. This enhancements saves an a large amount of time for each continuous attribute found within the datasets as descritization does not need to occur. Furthermore, this algorithm chooses the optimal threshhold to split the data which improves classification accuracy.

\subsection{Naïve Bayes}
The Naive Bayes algorithm is implemented as described in \cite{Mitchell:1997:ML:541177}. This algorithm is named due its assumption of conditional independance amongst each attribute:

$$P(a_1,...,a_n|v_j) = \prod_{i=1}^{n}P(a_i|v_j)$$

and its application of Bayes thereom:

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

Naive Bayes is a practical learning algorithm, receives relatively good results given its preference bias and is easy to implement. This algorithm naturally handles both catigorical and continuous data (given a prior is assumed). For each continuous attribute, a Gaussian distribution was assumed. The final algorithm implemented was:

$$\hat{y}= \underset{k\;\in\;\{1,..K\}} {\mathrm{argmax}} \;\;P(v_k)\prod_{i=1}^mp\left(a_i\;|\;v_k\right)\prod_{j=m}^nP\left(a_j\;|\;v_k\right)\;\;$$

where $P$ is the probability, $p$ is the gaussian probability density function and $K$ is the number of distinct targets. In this equation, the first m attributes are continous whereas the later are catigorical. During training, the data $S$ is split by lables into subsets ${S_1,...,S_n}$ where $n$ is the number of different classes. For each subset, we estimate the class probability $P(v_k)$. Subsequently, for each attribute value $a_i$ of each attribute $a$, we estimate $P(a_i|v_k)$ or $p$. We are not able to immidiately calculate $p(a_i|v_k)$ as attribute $a$ is a continous attribute.

\subsection{Adaboost}
\subsection{Random Forest}
\subsection{K-Nearest Neighbors}

\section{Data}
\subsection{Breast Cancer Data}
\subsection{Car Data}
Transformed to integer values

\subsection{E. Coli Data}
Removed categorical values

\subsection{Letter Recognition Data}
\subsection{Mushroom Data}

\section{Technical Details}
Identification information was removed ...
No discretization was performed ...
Unknown variables from the breast cancer dataset were replaced ...
Limitations placed on trees ...
Distributions used for Naïve Bayes ...
C4.5 continuous variable implementation ...
Number of trees for random forest ... & size of datasets
Number of weak learners for adaboost ... & size of datasets
How many neighbors ...

\section{Project Design}
A Java machine learning framework was first created ...
The external dependencies include JUnit for unit tests and slfj4 for logging. Both work independently of the code written for the machine learning algorithms. They functioned as tools to write code and may be completely removed if necessary ...

An `Algorithm` interface and `Model` abstract class were created as the high level objects that all algorithms and models implement. This abstraction proved extremely useful for ensemble implementation and general code cleanliness. The main class where the algorithms are implemented on the datasets is extremely easy to read.

A `KFold` object was created which implements the cross validation algorithm. This object generates a `Report` on a given algorithm and dataset ...

\subsection{Math Package}
Data was particularly difficult to handle. Whereas Java is a statically typed language, difficulties arose in handling both floating point and integer values. To manage this, a math package was created which implements useful data structures such as vectors and matrices. These object were used to abstract away from the underlying data structures. Furthermore, the `Vector` object provided many useful methods for vector arithmetic such as element-wise multiplication, division and exponents. These methods also support broadcasting, where a vector can be multiplied by a single number. These objects are used throughout the library in place of primitive arrays and ArrayLists and have were essential to the code quality. Often, methods it was necessary to have two associated objects held in one data structure. For example, there exists a method within the DataSet object which splits itself into two parts based on a pivot point. This method would preferably return a pair of datasets; however, many common data structures such as arrays, lists and maps seemed did not seem to adequate. To accomplish this task, a simple Tuple data structure was created.

Several other useful objects were implemented within the math package such as distance function objects, distribution objects and an object containing useful static methods. The purpose of these objects will be elaborated within Naïve Bayes package design section. The utility object implemented useful functions not available within the Java math package.

\subsection{Decision Tree Package}


\subsection{Naïve Bayes Package}
\subsection{Ensemble Package}
\subsection{Neighbors Package}

\section{Results}
table of results ...
graph of hyperparameter search ...

\bibliographystyle{plain}
\bibliography{references}
\end{document}
